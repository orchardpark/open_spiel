{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a54d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do independent Q-learning in Airline Seats, and play it against random.\n",
    "# RL is based on python/examples/independent_tabular_qlearning.py\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python import rl_tools\n",
    "from open_spiel.python.algorithms import tabular_qlearner\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "\n",
    "# Create the environment\n",
    "env = rl_environment.Environment(\"airline_seats\")\n",
    "num_players = env.num_players\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "# Create the agents\n",
    "agents = [\n",
    "    tabular_qlearner.QLearner(player_id=0, num_actions=num_actions),\n",
    "    random_agent.RandomAgent(1, num_actions, \"Entropy Master 2000\")\n",
    "]\n",
    "# Train the Q-learning agents in play vs random.\n",
    "ep_rewards = [0.0,0.0]\n",
    "wins = 0.0\n",
    "games = 0.0\n",
    "for cur_episode in range(5000):\n",
    "  time_step = env.reset()\n",
    "  while not time_step.last():\n",
    "    player_id = time_step.observations[\"current_player\"]\n",
    "    agent_output = agents[player_id].step(time_step)\n",
    "    time_step = env.step([agent_output.action])\n",
    "    ep_rewards[0] += time_step.rewards[0]\n",
    "    ep_rewards[1] += time_step.rewards[1]\n",
    "    if(ep_rewards[0] > ep_rewards[1]):\n",
    "      wins+=1\n",
    "    games+=1\n",
    "  if cur_episode % 1000 == 0:\n",
    "    print(f\"Episode {cur_episode}Rewards agent: {ep_rewards[0]/1000} - random: {ep_rewards[1]/1000} | win rate: {wins/games}\")\n",
    "    wins = 0.0\n",
    "    games = 0.0\n",
    "    ep_rewards = [0,0]\n",
    "    \n",
    "  # Episode is over, step all agents with final info state.\n",
    "  for agent in agents:\n",
    "    agent.step(time_step)\n",
    "print(\"Done!\")\n",
    "# Evaluate the Q-learning agent against a random agent.\n",
    "\n",
    "eval_agents = [agents[0], random_agent.RandomAgent(1, num_actions, \"Entropy Master 2000\") ]\n",
    "\n",
    "eval_rewards = [0.0, 0.0]\n",
    "for i in range(1000):\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "      player_id = time_step.observations[\"current_player\"]\n",
    "      # Note the evaluation flag. A Q-learner will set epsilon=0 here.\n",
    "      agent_output = eval_agents[player_id].step(time_step, is_evaluation=True)\n",
    "      time_step = env.step([agent_output.action])\n",
    "    eval_rewards[0] += time_step.rewards[0]\n",
    "    eval_rewards[1] += time_step.rewards[1]\n",
    "# Rewards over 1000 simulations\n",
    "print(f\"Agent: {eval_rewards[0]/1000} Random: {eval_rewards[1]/1000}\")\n",
    "\n",
    "# Play one game\n",
    "time_step = env.reset()\n",
    "while not time_step.last():\n",
    "  print(\"\")\n",
    "  print(env.get_state)\n",
    "  player_id = time_step.observations[\"current_player\"]\n",
    "  # Note the evaluation flag. A Q-learner will set epsilon=0 here.\n",
    "  agent_output = eval_agents[player_id].step(time_step, is_evaluation=True)\n",
    "  print(f\"Agent {player_id} chooses {env.get_state.action_to_string(agent_output.action)}\")\n",
    "  time_step = env.step([agent_output.action])\n",
    "print(env.get_state)\n",
    "print(time_step.rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
