{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a54d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 0 Reward: -4.88 Win ratio: 0.0\n",
      "Episodes: 1000 Reward: -3594.822 Win ratio: 0.469\n",
      "Episodes: 2000 Reward: -3514.349 Win ratio: 0.51\n",
      "Episodes: 3000 Reward: -3519.448 Win ratio: 0.531\n",
      "Episodes: 4000 Reward: -3547.399 Win ratio: 0.516\n",
      "Episodes: 5000 Reward: -3487.281 Win ratio: 0.508\n",
      "Episodes: 6000 Reward: -3551.246 Win ratio: 0.494\n",
      "Episodes: 7000 Reward: -3536.918 Win ratio: 0.507\n",
      "Episodes: 8000 Reward: -3518.177 Win ratio: 0.509\n",
      "Episodes: 9000 Reward: -3609.823 Win ratio: 0.489\n",
      "Episodes: 10000 Reward: -3500.971 Win ratio: 0.524\n",
      "Episodes: 11000 Reward: -3419.015 Win ratio: 0.538\n",
      "Episodes: 12000 Reward: -3529.369 Win ratio: 0.483\n",
      "Episodes: 13000 Reward: -3487.564 Win ratio: 0.494\n",
      "Episodes: 14000 Reward: -3501.737 Win ratio: 0.486\n",
      "Episodes: 15000 Reward: -3537.459 Win ratio: 0.497\n",
      "Episodes: 16000 Reward: -3466.366 Win ratio: 0.52\n",
      "Episodes: 17000 Reward: -3539.072 Win ratio: 0.512\n",
      "Episodes: 18000 Reward: -3400.523 Win ratio: 0.534\n",
      "Episodes: 19000 Reward: -3506.404 Win ratio: 0.494\n",
      "Episodes: 20000 Reward: -3555.325 Win ratio: 0.493\n",
      "Episodes: 21000 Reward: -3470.156 Win ratio: 0.53\n",
      "Episodes: 22000 Reward: -3691.321 Win ratio: 0.468\n",
      "Episodes: 23000 Reward: -3368.719 Win ratio: 0.539\n",
      "Episodes: 24000 Reward: -3521.496 Win ratio: 0.486\n",
      "Done!\n",
      "\n",
      "0|-0.267024|0|SB|0,0,||\n",
      "Agent 0 chooses Buy:81\n",
      "\n",
      "0|-0.267024|1|SB|81,0,||\n",
      "Agent 1 chooses Buy:69\n",
      "\n",
      "0|-0.267024|0|PS|81,69,||\n",
      "Agent 0 chooses SetPrice:108\n",
      "\n",
      "0|-0.267024|1|PS|81,69,||\n",
      "Agent 1 chooses SetPrice:136\n",
      "\n",
      "1|-0.267024|0|PS|81,69,|7,0,|108,136,\n",
      "Agent 0 chooses SetPrice:98\n",
      "\n",
      "1|-0.267024|1|PS|81,69,|7,0,|108,136,\n",
      "Agent 1 chooses SetPrice:99\n",
      "\n",
      "2|-0.267024|0|PS|81,69,|7,0,6,4,|108,136,98,99,\n",
      "Agent 0 chooses SetPrice:7\n",
      "\n",
      "2|-0.267024|1|PS|81,69,|7,0,6,4,|108,136,98,99,\n",
      "Agent 1 chooses SetPrice:14\n",
      "\n",
      "3|-0.267024|0|PS|81,69,|7,0,6,4,38,0,|108,136,98,99,7,14,\n",
      "Agent 0 chooses SetPrice:79\n",
      "\n",
      "3|-0.267024|1|PS|81,69,|7,0,6,4,38,0,|108,136,98,99,7,14,\n",
      "Agent 1 chooses SetPrice:125\n",
      "\n",
      "4|-0.267024|0|PS|81,69,|7,0,6,4,38,0,16,0,|108,136,98,99,7,14,79,125,\n",
      "Agent 0 chooses SetPrice:17\n",
      "\n",
      "4|-0.267024|1|PS|81,69,|7,0,6,4,38,0,16,0,|108,136,98,99,7,14,79,125,\n",
      "Agent 1 chooses SetPrice:25\n",
      "\n",
      "5|-0.267024|0|PS|81,69,|7,0,6,4,38,0,16,0,29,0,|108,136,98,99,7,14,79,125,17,25,\n",
      "Agent 0 chooses SetPrice:144\n",
      "\n",
      "5|-0.267024|1|PS|81,69,|7,0,6,4,38,0,16,0,29,0,|108,136,98,99,7,14,79,125,17,25,\n",
      "Agent 1 chooses SetPrice:80\n",
      "\n",
      "6|-0.267024|0|PS|81,69,|7,0,6,4,38,0,16,0,29,0,0,14,|108,136,98,99,7,14,79,125,17,25,144,80,\n",
      "Agent 0 chooses SetPrice:23\n",
      "\n",
      "6|-0.267024|1|PS|81,69,|7,0,6,4,38,0,16,0,29,0,0,14,|108,136,98,99,7,14,79,125,17,25,144,80,\n",
      "Agent 1 chooses SetPrice:22\n",
      "\n",
      "7|-0.267024|0|PS|81,69,|7,0,6,4,38,0,16,0,29,0,0,14,3,27,|108,136,98,99,7,14,79,125,17,25,144,80,23,22,\n",
      "Agent 0 chooses SetPrice:135\n",
      "\n",
      "7|-0.267024|1|PS|81,69,|7,0,6,4,38,0,16,0,29,0,0,14,3,27,|108,136,98,99,7,14,79,125,17,25,144,80,23,22,\n",
      "Agent 1 chooses SetPrice:115\n",
      "\n",
      "8|-0.267024|0|PS|81,69,|7,0,6,4,38,0,16,0,29,0,0,14,3,27,0,5,|108,136,98,99,7,14,79,125,17,25,144,80,23,22,135,115,\n",
      "Agent 0 chooses SetPrice:77\n",
      "\n",
      "8|-0.267024|1|PS|81,69,|7,0,6,4,38,0,16,0,29,0,0,14,3,27,0,5,|108,136,98,99,7,14,79,125,17,25,144,80,23,22,135,115,\n",
      "Agent 1 chooses SetPrice:8\n",
      "\n",
      "9|-0.267024|0|PS|81,69,|7,0,6,4,38,0,16,0,29,0,0,14,3,27,0,5,0,36,|108,136,98,99,7,14,79,125,17,25,144,80,23,22,135,115,77,8,\n",
      "Agent 0 chooses SetPrice:143\n",
      "\n",
      "9|-0.267024|1|PS|81,69,|7,0,6,4,38,0,16,0,29,0,0,14,3,27,0,5,0,36,|108,136,98,99,7,14,79,125,17,25,144,80,23,22,135,115,77,8,\n",
      "Agent 1 chooses SetPrice:138\n",
      "\n",
      "10|-0.267024|-4|PS|81,69,|7,0,6,4,38,0,16,0,29,0,0,14,3,27,0,5,0,36,0,-1,|108,136,98,99,7,14,79,125,17,25,144,80,23,22,135,115,77,8,143,138,\n",
      "[-2054.0, -1895.0]\n"
     ]
    }
   ],
   "source": [
    "# Let's do independent Q-learning in Tic-Tac-Toe, and play it against random.\n",
    "# RL is based on python/examples/independent_tabular_qlearning.py\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python import rl_tools\n",
    "from open_spiel.python.algorithms import tabular_qlearner\n",
    "\n",
    "# Create the environment\n",
    "env = rl_environment.Environment(\"airline_seats\")\n",
    "num_players = env.num_players\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "# Create the agents\n",
    "agents = [\n",
    "    tabular_qlearner.QLearner(player_id=idx, num_actions=num_actions)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "# Train the Q-learning agents in self-play.\n",
    "wins = 0.0\n",
    "lossties = 0.0\n",
    "ep_rewards = 0.0\n",
    "for cur_episode in range(25000):\n",
    "  time_step = env.reset()\n",
    "  while not time_step.last():\n",
    "    player_id = time_step.observations[\"current_player\"]\n",
    "    agent_output = agents[player_id].step(time_step)\n",
    "    time_step = env.step([agent_output.action])\n",
    "  if time_step.rewards[0] > time_step.rewards[1]:\n",
    "    wins += 1\n",
    "  else:\n",
    "    lossties += 1\n",
    "  ep_reward = time_step.rewards[0]\n",
    "  ep_rewards += ep_reward\n",
    "  if cur_episode %1000 == 0:\n",
    "    print(f\"Episodes: {cur_episode} Reward: {ep_rewards/1000} Win ratio: {wins/(wins+lossties)}\")\n",
    "    wins = 0.0\n",
    "    lossties = 0.0\n",
    "    ep_rewards = 0.0\n",
    "    \n",
    "  # Episode is over, step all agents with final info state.\n",
    "  for agent in agents:\n",
    "    agent.step(time_step)\n",
    "print(\"Done!\")\n",
    "# Evaluate the Q-learning agent against a random agent.\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "eval_agents = [agents[0], random_agent.RandomAgent(1, num_actions, \"Entropy Master 2000\") ]\n",
    "\n",
    "time_step = env.reset()\n",
    "while not time_step.last():\n",
    "  print(\"\")\n",
    "  print(env.get_state)\n",
    "  player_id = time_step.observations[\"current_player\"]\n",
    "  # Note the evaluation flag. A Q-learner will set epsilon=0 here.\n",
    "  agent_output = eval_agents[player_id].step(time_step, is_evaluation=True)\n",
    "  print(f\"Agent {player_id} chooses {env.get_state.action_to_string(agent_output.action)}\")\n",
    "  time_step = env.step([agent_output.action])\n",
    "\n",
    "print(\"\")\n",
    "print(env.get_state)\n",
    "print(time_step.rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
