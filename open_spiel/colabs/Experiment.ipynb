{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15a54d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 0 Reward: -1.74\n",
      "Episodes: 1000 Reward: -3508.891\n",
      "Episodes: 2000 Reward: -3600.432\n",
      "Episodes: 3000 Reward: -3560.059\n",
      "Episodes: 4000 Reward: -3512.851\n",
      "Episodes: 5000 Reward: -3488.582\n",
      "Episodes: 6000 Reward: -3565.956\n",
      "Episodes: 7000 Reward: -3512.771\n",
      "Episodes: 8000 Reward: -3423.055\n",
      "Episodes: 9000 Reward: -3489.543\n",
      "Episodes: 10000 Reward: -3406.598\n",
      "Episodes: 11000 Reward: -3488.776\n",
      "Episodes: 12000 Reward: -3400.617\n",
      "Episodes: 13000 Reward: -3528.313\n",
      "Episodes: 14000 Reward: -3580.066\n",
      "Episodes: 15000 Reward: -3579.763\n",
      "Episodes: 16000 Reward: -3532.979\n",
      "Episodes: 17000 Reward: -3587.932\n",
      "Episodes: 18000 Reward: -3549.46\n",
      "Episodes: 19000 Reward: -3472.792\n",
      "Episodes: 20000 Reward: -3515.662\n",
      "Episodes: 21000 Reward: -3469.883\n",
      "Episodes: 22000 Reward: -3541.295\n",
      "Episodes: 23000 Reward: -3632.977\n",
      "Episodes: 24000 Reward: -3522.108\n",
      "Done!\n",
      "Agent: -3707.077 Random: -3660.647\n"
     ]
    }
   ],
   "source": [
    "# Let's do independent Q-learning in Airline Seats, and play it against random.\n",
    "# RL is based on python/examples/independent_tabular_qlearning.py\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python import rl_tools\n",
    "from open_spiel.python.algorithms import tabular_qlearner\n",
    "\n",
    "# Create the environment\n",
    "env = rl_environment.Environment(\"airline_seats\")\n",
    "num_players = env.num_players\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "# Create the agents\n",
    "agents = [\n",
    "    tabular_qlearner.QLearner(player_id=idx, num_actions=num_actions)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "# Train the Q-learning agents in self-play.\n",
    "ep_rewards = 0.0\n",
    "for cur_episode in range(25000):\n",
    "  time_step = env.reset()\n",
    "  while not time_step.last():\n",
    "    player_id = time_step.observations[\"current_player\"]\n",
    "    agent_output = agents[player_id].step(time_step)\n",
    "    time_step = env.step([agent_output.action])\n",
    "  ep_reward = time_step.rewards[0]\n",
    "  ep_rewards += ep_reward\n",
    "  if cur_episode %1000 == 0:\n",
    "    print(f\"Episodes: {cur_episode} Reward: {ep_rewards/1000}\")\n",
    "    ep_rewards = 0.0\n",
    "    \n",
    "  # Episode is over, step all agents with final info state.\n",
    "  for agent in agents:\n",
    "    agent.step(time_step)\n",
    "print(\"Done!\")\n",
    "# Evaluate the Q-learning agent against a random agent.\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "eval_agents = [agents[0], random_agent.RandomAgent(1, num_actions, \"Entropy Master 2000\") ]\n",
    "\n",
    "eval_rewards = [0.0, 0.0]\n",
    "for i in range(1000):\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "      player_id = time_step.observations[\"current_player\"]\n",
    "      # Note the evaluation flag. A Q-learner will set epsilon=0 here.\n",
    "      agent_output = eval_agents[player_id].step(time_step, is_evaluation=True)\n",
    "      time_step = env.step([agent_output.action])\n",
    "    eval_rewards[0] += time_step.rewards[0]\n",
    "    eval_rewards[1] += time_step.rewards[1]\n",
    "\n",
    "print(f\"Agent: {eval_rewards[0]/1000} Random: {eval_rewards[1]/1000}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
